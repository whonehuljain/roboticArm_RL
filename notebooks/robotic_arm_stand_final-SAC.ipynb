{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46157ebe-eb7b-4cb4-86c9-7d02a4ff5305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0d23b32-3c1d-492c-b288-cfffd673a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoboticArmEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RoboticArmEnv, self).__init__()\n",
    "\n",
    "        self.steps_done = 0\n",
    "        self.max_steps = 200\n",
    "        self.action_space = spaces.Box(low=-np.pi, high=np.pi, shape=(2,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-np.pi, high=np.pi, shape=(4,), dtype=np.float32)\n",
    "        \n",
    "        self.angle1 = 0.0  \n",
    "        self.angle2 = 0.0  \n",
    "        self.length1 = 1.0  #main arm\n",
    "        self.length2 = 0.5  #upper arm\n",
    "        self.prev_angle1 = self.angle1\n",
    "        self.prev_angle2 = self.angle2\n",
    "        self.end_effector_x = self.length1 * np.cos(self.angle1) + self.length2 * np.cos(self.angle1 + self.angle2)\n",
    "        self.end_effector_y = self.length1 * np.sin(self.angle1) + self.length2 * np.sin(self.angle1 + self.angle2)\n",
    "        \n",
    "        self.last_action = np.array([0, 0]) \n",
    "\n",
    "        \n",
    "        self.reward_history = [] \n",
    "        self.smoothing_window_size = 20 \n",
    "        self.state = np.array([self.angle1, self.angle2, self.end_effector_x, self.end_effector_y], dtype=np.float32)\n",
    "\n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        if seed is not None:\n",
    "            self.np_random, _ = gym.utils.seeding.np_random(seed) \n",
    "\n",
    "        self.angle1 = np.random.uniform(0.5 * np.pi, 1.5 * np.pi)\n",
    "        self.angle2 = np.random.uniform(0.5 * np.pi, 1.5 * np.pi) \n",
    "\n",
    "        self.angle1 = np.clip(self.angle1, 0, np.pi)\n",
    "        self.angle2 = np.clip(self.angle2, 0, np.pi)\n",
    "\n",
    "        self.prev_angle1 = self.angle1\n",
    "        self.prev_angle2 = self.angle2\n",
    "        self.last_action = np.array([0, 0])\n",
    "\n",
    "        self.end_effector_x = self.length1 * np.cos(self.angle1) + self.length2 * np.cos(self.angle1 + self.angle2)\n",
    "        self.end_effector_y = self.length1 * np.sin(self.angle1) + self.length2 * np.sin(self.angle1 + self.angle2)\n",
    "\n",
    "        self.steps_done = 0 \n",
    "\n",
    "        self.state = np.array([self.angle1, self.angle2, self.end_effector_x, self.end_effector_y], dtype=np.float32)\n",
    "        \n",
    "        return self.state, {} \n",
    "\n",
    "\n",
    "    \n",
    "    def reward_function(self):\n",
    "        angle1_deviation = abs(self.angle1 - np.pi / 2)\n",
    "        combined_deviation = abs((self.angle1 + self.angle2) - np.pi / 2)\n",
    "        \n",
    "        angle1_penalty = -20 * (angle1_deviation ** 2)\n",
    "        combined_penalty = -15 * (combined_deviation ** 2)\n",
    "        stability_penalty = -5 * (abs(self.angle1 - self.prev_angle1) + abs(self.angle2 - self.prev_angle2))\n",
    "\n",
    "        steady_state_bonus = 1.5 if angle1_deviation < 0.01 and combined_deviation < 0.01 and self.steps_done > 50 else 0\n",
    "\n",
    "        action_penalty = -0.1 * (abs(self.last_action[0]) + abs(self.last_action[1]))\n",
    "\n",
    "        time_penalty = -1.0 * (angle1_deviation + combined_deviation) if self.steps_done > 50 else 0\n",
    "        hold_bonus = 1.0 if angle1_deviation < 0.02 and combined_deviation < 0.02 else 0\n",
    "        \n",
    "        total_reward = angle1_penalty + combined_penalty + stability_penalty + action_penalty + time_penalty + hold_bonus+steady_state_bonus\n",
    "        \n",
    "        scaled_reward = total_reward / (np.pi ** 2)\n",
    "    \n",
    "        self.reward_history.append(scaled_reward)\n",
    "        smoothed_reward = self.smooth_reward(self.reward_history, self.smoothing_window_size)\n",
    "        \n",
    "        return smoothed_reward\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def smooth_reward(self, reward_history, window_size=10):\n",
    "        if len(reward_history) < window_size:\n",
    "            return np.mean(reward_history)\n",
    "\n",
    "        return np.mean(reward_history[-window_size:])\n",
    "    \n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -0.05, 0.05)\n",
    "        \n",
    "        self.angle1 += action[0]\n",
    "        self.angle2 += action[1]\n",
    "\n",
    "        self.angle1 = np.clip(self.angle1, 0, np.pi)\n",
    "        self.angle2 = np.clip(self.angle2, 0, np.pi)\n",
    "\n",
    "        self.end_effector_x = self.length1 * np.cos(self.angle1) + self.length2 * np.cos(self.angle1 + self.angle2)\n",
    "        self.end_effector_y = self.length1 * np.sin(self.angle1) + self.length2 * np.sin(self.angle1 + self.angle2)\n",
    "\n",
    "        self.state = np.array([self.angle1, self.angle2, self.end_effector_x, self.end_effector_y], dtype=np.float32)\n",
    "        \n",
    "        reward = self.reward_function()\n",
    "\n",
    "        self.steps_done +=1\n",
    "\n",
    "        terminated = False\n",
    "        truncated = self.steps_done >= self.max_steps\n",
    "        \n",
    "        self.prev_angle1 = self.angle1\n",
    "        self.prev_angle2 = self.angle2\n",
    "\n",
    "        self.last_action = action\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, terminated, truncated, info\n",
    "        \n",
    "\n",
    "    def render(self, mode='human'):\n",
    "\n",
    "        if mode != 'human':\n",
    "            raise NotImplementedError(\"Only human mode is supported.\")\n",
    "            \n",
    "        \"\"\"Renders the environment using Pygame.\"\"\"\n",
    "        # Initialize Pygame if not already done\n",
    "        if not hasattr(self, 'screen'):\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((600, 600))\n",
    "            pygame.display.set_caption('Robotic Arm Simulation')\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        # Clear the screen\n",
    "        self.screen.fill((255, 255, 255))  # White background\n",
    "\n",
    "        # Define the center of the base\n",
    "        base_x, base_y = 300, 300  # Center of the window\n",
    "\n",
    "        # Calculate joint positions\n",
    "        joint1_x = base_x + int(self.length1 * 100 * np.cos(self.angle1))\n",
    "        joint1_y = base_y - int(self.length1 * 100 * np.sin(self.angle1))\n",
    "\n",
    "        joint2_x = joint1_x + int(self.length2 * 100 * np.cos(self.angle1 + self.angle2))\n",
    "        joint2_y = joint1_y - int(self.length2 * 100 * np.sin(self.angle1 + self.angle2))\n",
    "\n",
    "        # Draw the base\n",
    "        pygame.draw.circle(self.screen, (0, 0, 0), (base_x, base_y), 5)  # Black circle for the base\n",
    "\n",
    "        # Draw the first segment\n",
    "        pygame.draw.line(self.screen, (0, 0, 255), (base_x, base_y), (joint1_x, joint1_y), 5)  # Blue line\n",
    "\n",
    "        # Draw the second segment\n",
    "        pygame.draw.line(self.screen, (255, 0, 0), (joint1_x, joint1_y), (joint2_x, joint2_y), 5)  # Red line\n",
    "\n",
    "        # Draw the end-effector\n",
    "        pygame.draw.circle(self.screen, (0, 255, 0), (joint2_x, joint2_y), 5)  # Green circle for the end-effector\n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "\n",
    "        # Limit the frame rate\n",
    "        self.clock.tick(30)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Closes the Pygame window and releases resources.\"\"\"\n",
    "        if hasattr(self, 'screen'):\n",
    "            pygame.quit()\n",
    "            del self.screen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "709779e6-7527-4186-8c57-c936e88eda52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import pandas as pd\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3 import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c517e71e-b524-4039-862b-b384a428c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RoboticArmEnv()\n",
    "timed_env = TimeLimit(env, max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd4733a4-1ab3-4b4c-9b99-f5f93976ea75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.68e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 24        |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 46.6      |\n",
      "|    critic_loss     | 7.79      |\n",
      "|    ent_coef        | 0.0105    |\n",
      "|    ent_coef_loss   | 9.64      |\n",
      "|    learning_rate   | 5e-05     |\n",
      "|    n_updates       | 1745      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.16e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 53        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 54.4      |\n",
      "|    critic_loss     | 6.15      |\n",
      "|    ent_coef        | 0.0119    |\n",
      "|    ent_coef_loss   | 11.8      |\n",
      "|    learning_rate   | 5e-05     |\n",
      "|    n_updates       | 3745      |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -895     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 29       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 2400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 47.5     |\n",
      "|    critic_loss     | 5.71     |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | 13.7     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 5745     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -763     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 29       |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 3200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 41.3     |\n",
      "|    critic_loss     | 4.78     |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 12.7     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 7745     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -670     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 29       |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 36.9     |\n",
      "|    critic_loss     | 3.74     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 12.1     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 9745     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -613     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 28       |\n",
      "|    time_elapsed    | 165      |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.8     |\n",
      "|    critic_loss     | 3.42     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | 10.3     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 11745    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -574     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 28       |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 29.3     |\n",
      "|    critic_loss     | 3.79     |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | 10.1     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 13745    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -544     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 231      |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.4     |\n",
      "|    critic_loss     | 2.22     |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | 9.14     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 15745    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -499     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 25.8     |\n",
      "|    critic_loss     | 3.7      |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | 8.22     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 17745    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -494     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.7     |\n",
      "|    critic_loss     | 3.53     |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | 7.72     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 19745    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -485     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 320      |\n",
      "|    total_timesteps | 8800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26       |\n",
      "|    critic_loss     | 2.83     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | 6.49     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 21745    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -481     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 347      |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 27.8     |\n",
      "|    critic_loss     | 3.61     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | 6.28     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 23745    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -479     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 374      |\n",
      "|    total_timesteps | 10400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 28.2     |\n",
      "|    critic_loss     | 3.37     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 4.57     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 25745    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -466     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 402      |\n",
      "|    total_timesteps | 11200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 23.8     |\n",
      "|    critic_loss     | 2.99     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 4.48     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 27745    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -460     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 430      |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 24.6     |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.0419   |\n",
      "|    ent_coef_loss   | 4.03     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 29745    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -452     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 27       |\n",
      "|    time_elapsed    | 457      |\n",
      "|    total_timesteps | 12800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 26.1     |\n",
      "|    critic_loss     | 2.8      |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | 3.23     |\n",
      "|    learning_rate   | 5e-05    |\n",
      "|    n_updates       | 31745    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x7d0393dd7b90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "import numpy as np\n",
    "\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    timed_env,\n",
    "    verbose=1,\n",
    "    learning_rate=0.00005,\n",
    "    batch_size=128, \n",
    "    train_freq=2, \n",
    "    gradient_steps=5,\n",
    "    gamma=0.95, \n",
    "    tau=0.005,\n",
    "    ent_coef='auto_0.01'               \n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=13000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcae7a62-ff9e-45da-b2bf-2c1bcd923ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"rob_arm_SAC18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff3ec828-3ea6-4b83-bce9-85f46ce63c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f0333d4-b036-4089-8302-8078e29d4153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Episode 1: Score = -101.61\n",
      "Episode 2: Score = -514.86\n",
      "Episode 3: Score = -206.86\n",
      "Episode 4: Score = -387.78\n",
      "Episode 5: Score = -288.39\n"
     ]
    }
   ],
   "source": [
    "model = SAC.load(\"rob_arm_SAC18\", env=timed_env)\n",
    "\n",
    "for episode in range(5):\n",
    "    obs, _ = timed_env.reset()\n",
    "    score = 0\n",
    "    offset =0.3\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = timed_env.step(action)\n",
    "        timed_env.render()\n",
    "        reward+=offset\n",
    "        score += reward\n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode {episode + 1}: Score = {score:.2f}\")\n",
    "            break\n",
    "\n",
    "timed_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765eed66-8284-4730-9d6c-b58dd79f4284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
