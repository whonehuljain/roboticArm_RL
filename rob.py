import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time
from PIL import Image
import plotly.graph_objects as go
# import plotly.express as px
# import gymnasium as gym
# from gymnasium import spaces
# from stable_baselines3 import PPO
# from gym.wrappers import TimeLimit

# import streamlit as st
from stable_baselines3 import PPO
from gymnasium.wrappers import TimeLimit
import cv2
import RoboticArmEnv as RoboticArmEnv


# Page configuration
st.set_page_config(
    page_title="Robotic Arm Stand - ML Project Showcase",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Enhanced CSS for better dark mode compatibility
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        font-weight: bold;
    }
    .section-header {
        font-size: 2rem;
        color: #ff7f0e;
        margin-top: 2rem;
        margin-bottom: 1rem;
        border-bottom: 2px solid #ff7f0e;
        padding-bottom: 0.5rem;
    }
    .highlight-box {
        background-color: rgba(31, 119, 180, 0.1);
        color: var(--text-color);
        padding: 1rem;
        border-radius: 10px;
        border-left: 5px solid #1f77b4;
        margin: 1rem 0;
    }
    .success-box {
        background-color: rgba(50, 205, 50, 0.1);
        color: var(--text-color);
        padding: 1rem;
        border-radius: 10px;
        border-left: 5px solid #32cd32;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: rgba(255, 165, 0, 0.1);
        color: var(--text-color);
        padding: 1rem;
        border-radius: 10px;
        border-left: 5px solid #ffa500;
        margin: 1rem 0;
    }
    
    /* Dark mode compatibility */
    [data-theme="dark"] .highlight-box {
        background-color: rgba(31, 119, 180, 0.2);
        color: #ffffff;
    }
    [data-theme="dark"] .success-box {
        background-color: rgba(50, 205, 50, 0.2);
        color: #ffffff;
    }
    [data-theme="dark"] .warning-box {
        background-color: rgba(255, 165, 0, 0.2);
        color: #ffffff;
    }
    
    /* Ensure text visibility in both modes */
    .stMarkdown div {
        color: inherit;
    }
</style>
""", unsafe_allow_html=True)

# Your RoboticArmEnv class (exactly as provided)
# class RoboticArmEnv(gym.Env):
#     metadata = {'render.modes': ['human']}
    
#     def __init__(self):
#         super(RoboticArmEnv, self).__init__()

#         self.steps_done = 0
#         self.max_steps = 200
#         self.action_space = spaces.Box(low=-np.pi, high=np.pi, shape=(2,), dtype=np.float32)
#         self.observation_space = spaces.Box(low=-np.pi, high=np.pi, shape=(4,), dtype=np.float32)
        
#         self.angle1 = 0.0  
#         self.angle2 = 0.0  
#         self.length1 = 1.0  #main arm
#         self.length2 = 0.5  #upper arm
#         self.prev_angle1 = self.angle1
#         self.prev_angle2 = self.angle2
#         self.end_effector_x = self.length1 * np.cos(self.angle1) + self.length2 * np.cos(self.angle1 + self.angle2)
#         self.end_effector_y = self.length1 * np.sin(self.angle1) + self.length2 * np.sin(self.angle1 + self.angle2)
        
#         self.last_action = np.array([0, 0]) 

        
#         self.reward_history = [] 
#         self.smoothing_window_size = 20 
#         self.state = np.array([self.angle1, self.angle2, self.end_effector_x, self.end_effector_y], dtype=np.float32)

#     def reset(self, seed=None, **kwargs):
#         if seed is not None:
#             self.np_random, _ = gym.utils.seeding.np_random(seed) 

#         self.angle1 = np.random.uniform(0.5 * np.pi, 1.5 * np.pi)
#         self.angle2 = np.random.uniform(0.5 * np.pi, 1.5 * np.pi) 

#         self.angle1 = np.clip(self.angle1, 0, np.pi)
#         self.angle2 = np.clip(self.angle2, 0, np.pi)

#         self.prev_angle1 = self.angle1
#         self.prev_angle2 = self.angle2
#         self.last_action = np.array([0, 0])

#         self.end_effector_x = self.length1 * np.cos(self.angle1) + self.length2 * np.cos(self.angle1 + self.angle2)
#         self.end_effector_y = self.length1 * np.sin(self.angle1) + self.length2 * np.sin(self.angle1 + self.angle2)

#         self.steps_done = 0 

#         self.state = np.array([self.angle1, self.angle2, self.end_effector_x, self.end_effector_y], dtype=np.float32)
        
#         return self.state, {} 

#     def reward_function(self):
#         angle1_deviation = abs(self.angle1 - np.pi/2)
#         angle1_reward = -5*np.exp(angle1_deviation) 
        
#         if angle1_deviation < 0.02:
#             angle1_reward += 0.3

#         if angle1_deviation > 0.01:
#             correction_reward = -0.1 * angle1_deviation
#             angle1_reward += correction_reward
#         else:
#             correction_reward = 0
#             angle1_reward += correction_reward

#         combined_angle_deviation =abs((self.angle1 + self.angle2) - np.pi/2)
#         combined_angle_reward = -2 * (combined_angle_deviation ** 2) 
#         stability_reward=0.3 if angle1_deviation < 0.05 and combined_angle_deviation < 0.05 else 0
#         action_penalty=  -0.1 * (abs(self.last_action[0]) + abs(self.last_action[1]))
#         jerk_penalty = -0.3 * (abs(self.angle1 -self.prev_angle1) +abs(self.angle2 -self.prev_angle2))
#         adaptive_penalty= -1.5 * (np.exp(angle1_deviation) + np.exp(combined_angle_deviation)) if angle1_deviation > 0.1 else 0
            
#         if self.steps_done > 50:  
#             time_penalty= -0.5 * (angle1_deviation + combined_angle_deviation)
#         else:
#             time_penalty=0
    
#         total_reward= angle1_reward+combined_angle_reward+stability_reward+jerk_penalty+action_penalty+adaptive_penalty+time_penalty
    
#         scaled_reward =total_reward/(np.pi**2)
#         self.reward_history.append(scaled_reward)

#         smoothed_reward = self.smooth_reward(self.reward_history, self.smoothing_window_size)
        
#         return smoothed_reward

#     def smooth_reward(self, reward_history, window_size=10):
#         if len(reward_history) < window_size:
#             return np.mean(reward_history)

#         return np.mean(reward_history[-window_size:])

#     def step(self, action):
#         action = np.clip(action, -0.05, 0.05)
        
#         self.angle1 += action[0]
#         self.angle2 += action[1]

#         self.angle1 = np.clip(self.angle1, 0, np.pi)
#         self.angle2 = np.clip(self.angle2, 0, np.pi)

#         self.end_effector_x = self.length1 * np.cos(self.angle1) + self.length2 * np.cos(self.angle1 + self.angle2)
#         self.end_effector_y = self.length1 * np.sin(self.angle1) + self.length2 * np.sin(self.angle1 + self.angle2)

#         self.state = np.array([self.angle1, self.angle2, self.end_effector_x, self.end_effector_y], dtype=np.float32)
        
#         reward = self.reward_function()

#         self.steps_done +=1

#         terminated = False
#         truncated = self.steps_done >= self.max_steps
        
#         self.prev_angle1 = self.angle1
#         self.prev_angle2 = self.angle2

#         self.last_action = action

#         info = {}

#         return self.state, reward, terminated, truncated, info

def main():
    # Sidebar navigation
    st.sidebar.title("ü§ñ Navigation")
    page = st.sidebar.selectbox(
        "Choose a section:",
        ["üè† Project Overview", "üéØ Problem Statement", "üî¨ Technical Approach", 
         "‚öôÔ∏è Implementation Details", "üìä Results & Analysis", "üéÆ Live Simulation", "üéâ Conclusion"]
    )
    
    if page == "üè† Project Overview":
        show_overview()
    elif page == "üéØ Problem Statement":
        show_problem_statement()
    elif page == "üî¨ Technical Approach":
        show_technical_approach()
    elif page == "‚öôÔ∏è Implementation Details":
        show_implementation()
    elif page == "üìä Results & Analysis":
        show_results()
    elif page == "üéÆ Live Simulation":
        show_simulation()
    elif page == "üéâ Conclusion":
        show_conclusion()

def show_overview():
    st.markdown('<h1 class="main-header">ü§ñ Robotic Arm Stand Project</h1>', unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        # Create a simple robotic arm visualization
        fig_overview = go.Figure()
        
        # Base
        fig_overview.add_trace(go.Scatter(
            x=[0], y=[0],
            mode='markers',
            marker=dict(size=15, color='black'),
            name='Base'
        ))
        
        # Target position (90 degrees)
        angle1_target = np.pi/2
        angle2_target = 0
        x1_target = 1.0 * np.cos(angle1_target)
        y1_target = 1.0 * np.sin(angle1_target)
        x2_target = x1_target + 0.5 * np.cos(angle1_target + angle2_target)
        y2_target = y1_target + 0.5 * np.sin(angle1_target + angle2_target)
        
        # First link
        fig_overview.add_trace(go.Scatter(
            x=[0, x1_target], y=[0, y1_target],
            mode='lines+markers',
            line=dict(color='blue', width=8),
            marker=dict(size=10),
            name='Main Arm'
        ))
        
        # Second link
        fig_overview.add_trace(go.Scatter(
            x=[x1_target, x2_target], y=[y1_target, y2_target],
            mode='lines+markers',
            line=dict(color='red', width=8),
            marker=dict(size=10),
            name='Upper Arm'
        ))
        
        fig_overview.update_layout(
            title="Target Position: 90¬∞ Upright Stand",
            xaxis_title="X Position",
            yaxis_title="Y Position",
            xaxis=dict(range=[-1.5, 1.5]),
            yaxis=dict(range=[-0.5, 2]),
            showlegend=True,
            height=400
        )
        
        st.plotly_chart(fig_overview, use_container_width=True)
    
    st.markdown("""
    <div class="highlight-box">
    <h3>üéØ Project Mission</h3>
    <p>Train a machine learning model to control a simulated robotic arm, making it stand upright at 90¬∞ and maintain perfect stability using reinforcement learning algorithms.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Key achievements with actual data
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("üéØ Target Angle", "90¬∞", "Perfect Upright")
    
    with col2:
        st.metric("üéØ Final Performance", "-336 avg reward", "Stable")
    
    with col3:
        st.metric("ü§ñ Algorithm Used", "PPO", "Optimal Choice")
    
    with col4:
        st.metric("‚ö° Training Steps", "105K", "Converged")

def show_problem_statement():
    st.markdown('<h1 class="main-header">üéØ The Challenge</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="highlight-box">
    <h3>ü§î What Problem Are We Solving?</h3>
    <p>The challenge is to train a robotic arm with two joints to stand perfectly upright (90¬∞) and maintain stability. This involves:</p>
    <ul>
        <li><strong>Two-Joint Control:</strong> Main arm (length 1.0) and upper arm (length 0.5)</li>
        <li><strong>Continuous Actions:</strong> Small incremental movements (-0.05 to 0.05 radians)</li>
        <li><strong>Complex Reward Function:</strong> 7 different reward components for optimal behavior</li>
        <li><strong>Stability:</strong> Must maintain position without oscillating</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)
    
    # Environment specifications
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        <div class="highlight-box">
        <h4>üéÆ Environment Specs</h4>
        <ul>
            <li><strong>Action Space:</strong> Box(2,) - Joint torques</li>
            <li><strong>Observation Space:</strong> Box(4,) - [angle1, angle2, end_x, end_y]</li>
            <li><strong>Episode Length:</strong> 200 steps maximum</li>
            <li><strong>Action Clipping:</strong> ¬±0.05 radians per step</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="warning-box">
        <h4>‚ö†Ô∏è Key Challenges</h4>
        <ul>
            <li><strong>Sparse Rewards:</strong> Only rewarded when close to target</li>
            <li><strong>Continuous Control:</strong> Smooth movements required</li>
            <li><strong>Multi-objective:</strong> Speed vs stability trade-off</li>
            <li><strong>Local Minima:</strong> Avoiding suboptimal solutions</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

def show_technical_approach():
    st.markdown('<h1 class="main-header">üî¨ Technical Approach</h1>', unsafe_allow_html=True)
    
    st.markdown('<h2 class="section-header">üéÅ Reward Function Design</h2>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="highlight-box">
    <h3>Complex 7-Component Reward System</h3>
    <p>The reward function is the heart of this project, consisting of 7 carefully designed components:</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Reward components breakdown
    reward_components = [
        {
            "Component": "Angle1 Reward",
            "Formula": "-5 * exp(|angle1 - œÄ/2|)",
            "Purpose": "Primary guidance toward 90¬∞",
            "Impact": "High"
        },
        {
            "Component": "Combined Angle Reward", 
            "Formula": "-2 * (|(angle1 + angle2) - œÄ/2|)¬≤",
            "Purpose": "Ensure total arm points upward",
            "Impact": "High"
        },
        {
            "Component": "Stability Bonus",
            "Formula": "+0.3 if deviations < 0.05",
            "Purpose": "Reward staying close to target",
            "Impact": "Medium"
        },
        {
            "Component": "Correction Reward",
            "Formula": "-0.1 * deviation if > 0.01",
            "Purpose": "Guide back when drifting",
            "Impact": "Medium"
        },
        {
            "Component": "Jerk Penalty",
            "Formula": "-0.3 * |Œîangle1| + |Œîangle2|",
            "Purpose": "Reduce sudden movements",
            "Impact": "High"
        },
        {
            "Component": "Action Penalty",
            "Formula": "-0.1 * (|action1| + |action2|)",
            "Purpose": "Prevent extreme actions",
            "Impact": "Low"
        },
        {
            "Component": "Adaptive Penalty",
            "Formula": "-1.5 * exp(deviations) if > 0.1",
            "Purpose": "Strong penalty for large errors",
            "Impact": "High"
        }
    ]
    
    df_rewards = pd.DataFrame(reward_components)
    st.dataframe(df_rewards, use_container_width=True)
    
    # PPO Configuration
    st.markdown("---")
    st.markdown('<h2 class="section-header">üöÄ PPO Configuration</h2>', unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        <div class="success-box">
        <h4>üîß Hyperparameters</h4>
        <ul>
            <li><strong>Learning Rate:</strong> 0.00005</li>
            <li><strong>Gamma (Discount):</strong> 0.99</li>
            <li><strong>Entropy Coefficient:</strong> 0.01</li>
            <li><strong>Clip Range:</strong> 0.2</li>
            <li><strong>Policy:</strong> MlpPolicy</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="highlight-box">
        <h4>üéØ Why PPO?</h4>
        <ul>
            <li><strong>Stability:</strong> Conservative policy updates</li>
            <li><strong>Sample Efficiency:</strong> Good for continuous control</li>
            <li><strong>Robustness:</strong> Less sensitive to hyperparameters</li>
            <li><strong>Proven:</strong> Excellent for robotics applications</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

def show_implementation():
    st.markdown('<h1 class="main-header">‚öôÔ∏è Implementation Details</h1>', unsafe_allow_html=True)
    
    # Environment Code
    st.markdown('<h2 class="section-header">üåç Custom Environment</h2>', unsafe_allow_html=True)
    
    st.code("""
class RoboticArmEnv(gym.Env):
    def __init__(self):
        self.action_space = spaces.Box(low=-np.pi, high=np.pi, shape=(2,), dtype=np.float32)
        self.observation_space = spaces.Box(low=-np.pi, high=np.pi, shape=(4,), dtype=np.float32)
        
        self.length1 = 1.0  # main arm
        self.length2 = 0.5  # upper arm
        self.max_steps = 200
        
    def reward_function(self):
        # 7-component reward system
        angle1_deviation = abs(self.angle1 - np.pi/2)
        angle1_reward = -5*np.exp(angle1_deviation)
        
        # ... (additional reward components)
        
        total_reward = (angle1_reward + combined_angle_reward + 
                       stability_reward + jerk_penalty + action_penalty + 
                       adaptive_penalty + time_penalty)
        
        return total_reward / (np.pi**2)  # Scaling
    """, language="python")
    
    # Training Code
    st.markdown("---")
    st.markdown('<h2 class="section-header">üèãÔ∏è Training Process</h2>', unsafe_allow_html=True)
    
    st.code("""
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor

# Environment setup
env = RoboticArmEnv()
env = Monitor(env, './training_log')

# PPO model
model = PPO(
    "MlpPolicy", env, verbose=1,
    learning_rate=0.00005,
    gamma=0.99,
    ent_coef=0.01,
    clip_range=0.2
)

# Training
model.learn(total_timesteps=105000)
model.save("rob_arm_PPO")
    """, language="python")

def show_results():
    st.markdown('<h1 class="main-header">üìä Results & Analysis</h1>', unsafe_allow_html=True)
    
    # Actual training data from your results
    actual_rewards = [-2062.404508, -1591.6971626, -1100.9695897999998, -889.35849215, -781.5976947500001, 
                     -737.4635495, -758.5542697000002, -705.6224887000001, -681.13664795, -637.1993570000001, 
                     -645.7494906000001, -608.2601927000001, -557.4979827999999, -546.4227992000001, -522.4133091000001, 
                     -426.27094189999997, -364.7796043, -396.54009765, -382.42703685, -341.4713137, -376.60178720000005, 
                     -362.9526825000001, -355.06617014999995, -381.15111265, -360.79915475, -329.41065109999994, -336.12832583333335]
    
    episodes = list(range(1, len(actual_rewards) + 1))
    
    # Training progress chart
    st.markdown('<h2 class="section-header">üìà Training Progress</h2>', unsafe_allow_html=True)
    
    fig_training = go.Figure()
    fig_training.add_trace(go.Scatter(
        x=episodes,
        y=actual_rewards,
        mode='lines+markers',
        name='Mean Episode Reward (20-episode chunks)',
        line=dict(color='blue', width=3),
        marker=dict(size=6)
    ))
    
    fig_training.update_layout(
        title="Actual Training Progress - PPO Learning Curve",
        xaxis_title="Episode Chunk (20 episodes each)",
        yaxis_title="Mean Reward",
        height=500,
        showlegend=True
    )
    
    st.plotly_chart(fig_training, use_container_width=True)
    
    # Key insights
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("üéØ Initial Reward", f"{actual_rewards[0]:.0f}", "Starting Point")
    
    with col2:
        st.metric("üèÜ Final Reward", f"{actual_rewards[-1]:.0f}", f"{actual_rewards[-1] - actual_rewards[0]:+.0f}")
    
    with col3:
        st.metric("üìà Best Performance", f"{max(actual_rewards):.0f}", "Peak Achievement")
    
    with col4:
        st.metric("üìä Improvement", f"{((actual_rewards[-1] - actual_rewards[0]) / abs(actual_rewards[0]) * 100):.1f}%", "Total Progress")
    
    # Analysis
    st.markdown("---")
    st.markdown('<h2 class="section-header">üîç Training Analysis</h2>', unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        <div class="success-box">
        <h4>‚úÖ Key Observations</h4>
        <ul>
            <li><strong>Steady Improvement:</strong> Reward improved from -2062 to -336</li>
            <li><strong>Convergence:</strong> Model stabilized around episode 16-20</li>
            <li><strong>83% Improvement:</strong> Significant learning achieved</li>
            <li><strong>Stable Performance:</strong> Final episodes show consistent behavior</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="highlight-box">
        <h4>üìä Performance Phases</h4>
        <ul>
            <li><strong>Phase 1 (Episodes 1-5):</strong> Rapid initial learning</li>
            <li><strong>Phase 2 (Episodes 6-15):</strong> Gradual optimization</li>
            <li><strong>Phase 3 (Episodes 16-27):</strong> Fine-tuning and stability</li>
            <li><strong>Final State:</strong> Consistent upright positioning</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)

def show_simulation():
    st.markdown('<h1 class="main-header">üéÆ Live Model Simulation</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="highlight-box">
    <h3>üöÄ Experience the Trained Model!</h3>
    <p>This simulation demonstrates the trained PPO model controlling the robotic arm to maintain an upright position at 90¬∞.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Note about the simulation
    st.markdown("""
    <div class="warning-box">
    <h4>üí° Simulation Note</h4>
    <p>To run the actual trained model simulation with pygame rendering, use the following code in your local environment:</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.code("""
# Load your trained model and run simulation
env = RoboticArmEnv()
model = PPO.load("rob_arm_PPO")
timed_env = TimeLimit(env, max_episode_steps=200)

for episode in range(5):
    obs, _ = timed_env.reset(seed=42)
    score = 0
    offset = 0.3
    
    done = False
    while not done:
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = timed_env.step(action)
        timed_env.render()  # This opens pygame window
        reward += offset
        score += reward  
        if terminated or truncated:
            print(f"Episode {episode + 1}: Score = {score:.2f}")
            break  
            
timed_env.close()
    """, language="python")
    
    # Theoretical demonstration
    st.markdown("---")
    st.markdown("### ü§ñ Simulated Behavior Demonstration")
    
    # Create a demonstration of what the trained model does
    if st.button("üéØ Show Model Behavior", type="primary"):
        # Simulate the trained model's behavior
        # progress_bar = st.progress(0)
        # status_text = st.empty()
        
        # # Create placeholder for visualization
        # chart_placeholder = st.empty()
        
        # # Simulate 100 steps of the trained model
        # angles_over_time = []
        # target_angle = 90  # degrees
        
        # for step in range(100):
        #     # Simulate the model's learned behavior (converging to 90 degrees)
        #     if step < 20:
        #         # Initial adjustment phase
        #         current_angle = 45 + (45 * step / 20) + np.random.normal(0, 5)
        #     else:
        #         # Stable phase around 90 degrees
        #         current_angle = 90 + np.random.normal(0, 2)
            
        #     angles_over_time.append(current_angle)
            
        #     # Update progress
        #     progress_bar.progress((step + 1) / 100)
        #     status_text.text(f"Step {step + 1}/100 - Current Angle: {current_angle:.1f}¬∞")
            
        #     # Update chart
        #     fig = go.Figure()
        #     fig.add_trace(go.Scatter(
        #         x=list(range(len(angles_over_time))),
        #         y=angles_over_time,
        #         mode='lines',
        #         name='Arm Angle',
        #         line=dict(color='blue', width=2)
        #     ))
        #     fig.add_hline(y=90, line_dash="dash", line_color="red", 
        #                  annotation_text="Target: 90¬∞")
        #     fig.update_layout(
        #         title="Trained Model: Arm Angle Over Time",
        #         xaxis_title="Time Steps",
        #         yaxis_title="Angle (degrees)",
        #         yaxis=dict(range=[0, 180]),
        #         height=400
        #     )
            
        #     chart_placeholder.plotly_chart(fig, use_container_width=True)
        #     time.sleep(0.05)

        model_path = "trained_models/rob_arm_PPO"
        model = PPO.load(model_path)
        env = RoboticArmEnv.RoboticArmEnv()
        timed_env = TimeLimit(env, max_episode_steps=200)
        image_placeholder = st.empty()

        for episode in range(5):
            obs, _ = timed_env.reset(seed=42)
            done = False
            frames = []

            while not done:
                action, _states = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = timed_env.step(action)
                frame = timed_env.render() 
                
                frames.append(frame)

                if terminated or truncated:
                    st.write(f"Episode {episode + 1} finished.")
                    break

            for frame in frames:
                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                image_placeholder.image(frame, channels="BGR", use_column_width=True)

        timed_env.close()
        
        st.success("‚úÖ Simulation Complete! The model successfully maintains the arm at ~90¬∞")

def show_conclusion():
    st.markdown('<h1 class="main-header">üéâ Project Conclusion</h1>', unsafe_allow_html=True)
    
    # Key achievements
    st.markdown('<h2 class="section-header">üèÜ Key Achievements</h2>', unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        <div class="success-box">
        <h4>‚úÖ Technical Successes</h4>
        <ul>
            <li><strong>83% Performance Improvement:</strong> From -2062 to -336 reward</li>
            <li><strong>Stable Control:</strong> Achieved consistent 90¬∞ positioning</li>
            <li><strong>Complex Reward Design:</strong> 7-component reward system</li>
            <li><strong>Robust Training:</strong> 105K timesteps to convergence</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="highlight-box">
        <h4>üîß Technical Implementation</h4>
        <ul>
            <li><strong>Custom Gym Environment:</strong> Built from scratch</li>
            <li><strong>PPO Algorithm:</strong> Optimal choice for continuous control</li>
            <li><strong>Reward Engineering:</strong> Multi-objective optimization</li>
            <li><strong>Pygame Visualization:</strong> Real-time rendering</li>
        </ul>
        </div>
        """, unsafe_allow_html=True)
    
    # Final thoughts
    st.markdown("---")
    st.markdown("""
    <div class="success-box">
    <h3>üéØ Project Impact</h3>
    <p>This project successfully demonstrates the application of reinforcement learning to robotic control problems. The key insight was that <strong>reward function design is crucial</strong> - the 7-component reward system enabled the model to learn complex behaviors including stability, precision, and smooth movements.</p>
    
    <p>The 83% improvement in performance shows that PPO can effectively learn continuous control policies for multi-joint robotic systems, making this approach viable for real-world applications.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Thank you
    st.markdown("---")
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.markdown("""
        <div style="text-align: center; padding: 2rem;">
        <h3>üôè Thank You for Exploring!</h3>
        <p>This robotic arm control project showcases the power of reinforcement learning in solving complex continuous control problems.</p>
        </div>
        """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
